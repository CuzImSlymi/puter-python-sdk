name: Performance Monitoring

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 12 * * 0'  # Weekly on Sunday at noon

jobs:
  performance-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.9", "3.11", "3.12"]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest pytest-benchmark pytest-timeout memory-profiler

    - name: Run performance benchmarks
      run: |
        echo "Running basic performance tests..."
        python -c "
        import time
        from puter import PuterAI

        # Test basic initialization performance
        start = time.time()
        for i in range(100):
            client = PuterAI(username='test', password='test')
            models = client.get_available_models()
        end = time.time()

        print(f'100 client initializations took {end - start:.2f} seconds')
        print(f'Average per initialization: {(end - start) / 100 * 1000:.2f} ms')

        # Test model switching performance
        client = PuterAI(username='test', password='test')
        models = client.get_available_models()
        if len(models) >= 2:
            start = time.time()
            for i in range(50):
                client.set_model(models[i % min(2, len(models))])
            end = time.time()
            print(f'50 model switches took {end - start:.2f} seconds')
        "

  memory-profiling:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install memory-profiler psutil

    - name: Run memory profiling
      run: |
        python -c "
        import psutil
        import os
        from puter import PuterAI

        def test_memory_usage():
            print(f'Initial memory: {psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024:.2f} MB')

            # Test client initialization
            client = PuterAI(username='test', password='test')
            print(f'After client init: {psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024:.2f} MB')

            # Test model loading
            models = client.get_available_models()
            print(f'After model loading: {psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024:.2f} MB')
            print(f'Loaded {len(models)} models')

            # Test multiple client instances
            clients = [PuterAI(username='test', password='test') for _ in range(10)]
            print(f'After 10 clients: {psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024:.2f} MB')

            return len(models)

        result = test_memory_usage()
        print(f'Test completed successfully with {result} models')
        " > memory-profile-report.txt

    - name: Upload memory profile
      uses: actions/upload-artifact@v4
      with:
        name: memory-profile-report
        path: memory-profile-report.txt

  load-testing:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'push'
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .

    - name: Run load test
      run: |
        python -c "
        import time
        import threading
        import statistics
        from puter import PuterAI

        def simulate_user_session(results):
            start_time = time.time()
            try:
                client = PuterAI(username='test_user', password='test_pass')
                models = client.get_available_models()

                if models:
                    client.set_model(models[0])

                client.clear_chat_history()

                results.append({
                    'success': True,
                    'duration': time.time() - start_time,
                    'models_loaded': len(models)
                })
            except Exception as e:
                results.append({
                    'success': False,
                    'duration': time.time() - start_time,
                    'error': str(e)
                })

        def run_load_test(num_users=20):
            print(f'Starting load test with {num_users} users...')

            results = []
            threads = []

            for _ in range(num_users):
                thread = threading.Thread(target=simulate_user_session, args=(results,))
                threads.append(thread)
                thread.start()

            for thread in threads:
                thread.join()

            successful = [r for r in results if r['success']]
            failed = [r for r in results if not r['success']]

            print(f'Total sessions: {len(results)}')
            print(f'Successful: {len(successful)} ({len(successful)/len(results)*100:.1f}%)')
            print(f'Failed: {len(failed)} ({len(failed)/len(results)*100:.1f}%)')

            if successful:
                durations = [r['duration'] for r in successful]
                print(f'Average duration: {statistics.mean(durations):.2f}s')
                print(f'Median duration: {statistics.median(durations):.2f}s')

            return len(successful) / len(results) >= 0.95

        success = run_load_test()
        if not success:
            print('Load test failed to meet success criteria')
            exit(1)
        else:
            print('Load test passed!')
        " > load-test-report.txt
      continue-on-error: true

    - name: Upload load test report
      uses: actions/upload-artifact@v4
      with:
        name: load-test-report
        path: load-test-report.txt
